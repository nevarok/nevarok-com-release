{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Material for MkDocs","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"contacts/","title":"Contact Us","text":"<p>We value your feedback and are here to assist you. Feel free to reach out to us through any of the following channels:</p> <ul> <li> Email: contact@nevarok.com</li> <li> Discord: NevarokML Discord Server</li> <li> GitHub: NevarokML GitHub Repository</li> <li> YouTube: NevarokML YouTube Channel</li> <li> Unreal Marketplace: NevarokML on Unreal Marketplace</li> <li> Other:<ul> <li>Report a Bug</li> <li>Report a Documentation Issue</li> <li>Request a Change or a Feature</li> </ul> </li> </ul> <p>Please don't hesitate to reach out to us with any questions, suggestions, or inquiries. We'll be happy to assist you!</p> <p>Note</p> <p>The above contact information is provided for NevarokML and its associated platforms. For specific support or inquiries related to Unreal Engine or the Unreal Marketplace, please refer to their respective support channels.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#test-blog","title":"Test Blog","text":"<p>   2022-09-12 by nevarok </p> <p>Test Blog </p>  Continue reading  <p></p>"},{"location":"blog/posts/test-blog/","title":"Test Blog","text":"<p>Test Blog</p>"},{"location":"contributing/","title":"Community and Support","text":"<p>NevarokML is an actively maintained and constantly evolving project that values contributions from its diverse user base. We welcome bug reports, feature requests, documentation improvements, and community engagement to enhance the plugin's functionality and usability.</p>"},{"location":"contributing/#how-to-contribute","title":"How to contribute","text":"<p>We have streamlined the process for reporting issues, suggesting changes, and engaging with the NevarokML community. Here's how you can contribute:</p>  By actively participating in the NevarokML project, you contribute to its growth and help create a better experience for all users. Join us in shaping the future of machine learning in Unreal Engine."},{"location":"contributing/#something-is-not-working","title":"Something is not working?","text":"<p> Report a bug: If you encounter any issues or unexpected behavior while using NevarokML, create an issue on our bug tracker and provide a detailed description along with steps to reproduce the problem.</p>"},{"location":"contributing/#missing-information-in-our-docs","title":"Missing information in our docs?","text":"<p> Report a docs issue: If you find any gaps or inconsistencies in our documentation, feel free to report them. Your feedback helps us improve the quality and clarity of our documentation.</p>"},{"location":"contributing/#want-to-submit-an-idea","title":"Want to submit an idea?","text":"<p> Request a change: If you have an idea for a new feature, improvement, or change in NevarokML, submit a change request. We value your input and actively review and consider all requests.</p>"},{"location":"contributing/#have-a-question-or-need-help","title":"Have a question or need help?","text":"<p> Ask a question: If you have questions about NevarokML or need assistance, our community discussion board is a great place to seek help. Engage with the community and get answers to your queries.</p>"},{"location":"contributing/#join-our-discord-community","title":"Join our Discord community","text":"<p> Join Discord: We also encourage you to join our Discord community, where you can connect with other NevarokML users, discuss ideas, share knowledge, and collaborate on projects. Our friendly community is always ready to provide support and guidance</p>"},{"location":"contributing/reporting-a-bug/","title":"Reporting a bug","text":"<p>NevarokML is an actively maintained project, and we appreciate your help in identifying and reporting any bugs you encounter. To report a bug, please follow the steps below:</p> <p>We appreciate your efforts in creating a high-quality bug report, and our team will review and address the issue accordingly. Thank you for contributing to the improvement of NevarokML!</p>"},{"location":"contributing/reporting-a-bug/#before-creating-an-issue","title":"Before creating an issue","text":""},{"location":"contributing/reporting-a-bug/#upgrade-to-the-latest-version","title":"Upgrade to the latest version","text":"<ul> <li>Before reporting a bug, ensure that you are using the most recent version of NevarokML. Check for any available updates and upgrade if necessary.</li> </ul>"},{"location":"contributing/reporting-a-bug/#search-for-solutions","title":"Search for solutions","text":"<ul> <li>Prior to creating a bug report, search our documentation, issue tracker, and discussions board to see if the bug has already been reported or if there are any known workarounds or solutions available.</li> </ul>"},{"location":"contributing/reporting-a-bug/#create-a-bug-report","title":"Create a bug report","text":"<p>If you have followed the above steps and still encounter a bug, create an issue in our public issue tracker. Provide a clear and concise description of the bug, along with any relevant links to documentation, discussions, or search results related to the issue.</p>"},{"location":"contributing/reporting-a-bug/#reproduction","title":"Reproduction","text":"<ul> <li>Include a minimal reproduction of the bug, preferably as a .zip file. The reproduction should be small and focused, allowing us to quickly identify and reproduce the issue.</li> </ul>"},{"location":"contributing/reporting-a-bug/#steps-to-reproduce","title":"Steps to reproduce","text":"<ul> <li>Clearly outline the steps required to reproduce the bug using the provided reproduction. Keep the steps concise and easy to follow, ensuring continuity in the process.</li> </ul>"},{"location":"contributing/reporting-a-bug/#checklist","title":"Checklist","text":"<ul> <li>Before submitting the bug report, review the checklist to ensure you have followed all the guidelines and provided all the necessary information.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/","title":"Reporting a documentation issue","text":"<p>If you have identified an inconsistency, lack of clarity, or room for improvement in our documentation, we appreciate your effort in helping us maintain the quality of our documentation. To report a documentation issue, please follow the steps below:</p>  We appreciate your commitment to improving our documentation. Our team will review the issue and work towards resolving the inconsistency or making the necessary improvements. Thank you for your contribution!"},{"location":"contributing/reporting-a-docs-issue/#report-a-documentation-issue","title":"Report a documentation issue","text":"<p>Please thoroughly read the following guide before creating a new documentation issue, and provide the following information as part of the issue:</p>"},{"location":"contributing/reporting-a-docs-issue/#title","title":"Title","text":"<ul> <li>Create a concise and informative title that accurately describes the documentation issue. Include relevant keywords to facilitate searching in the issue tracker.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#description","title":"Description","text":"<ul> <li>Provide a clear and concise summary of the inconsistency or issue you encountered in the documentation. Explain why you believe the documentation should be adjusted and describe the severity of the issue. Keep the description brief and to the point.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#related-links","title":"Related links","text":"<ul> <li>Share the links to the specific documentation section that needs adjustment or improvement. If applicable, include any related links or anchor links (permanent links) to other relevant sections. This helps us understand the context and identify areas for improvement.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#proposed-change-optional","title":"Proposed change (optional)","text":"<ul> <li>If you have specific ideas or proposals for improving the documentation, you can provide them in this section. It can be a rough outline or a concrete proposal. While this field is optional, your suggestions can be valuable for both maintainers and the community.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#checklist","title":"Checklist","text":"<p>Before submitting the issue, review the checklist to ensure you have followed all the guidelines and provided all the necessary information.</p>"},{"location":"contributing/requesting-a-change/","title":"Requesting a change for NevarokML","text":"<p>Thank you for considering submitting a change request for NevarokML. We appreciate your interest in contributing to the project and want to ensure that your proposed change aligns with our guidelines and benefits the community. Before creating a change request, please review the following guidelines to help us understand your idea and evaluate its potential implementation.</p> <p>We look forward to reviewing your change request and working together to improve NevarokML.</p>"},{"location":"contributing/requesting-a-change/#before-creating-an-issue","title":"Before creating an issue","text":"<p>Before investing your time in submitting a change request, please answer the following questions to determine if your idea is suitable for NevarokML and aligns with the project's philosophy and goals:</p>"},{"location":"contributing/requesting-a-change/#purpose-of-the-change","title":"Purpose of the change","text":"<ul> <li>Change requests should focus on suggesting improvements, new features, or addressing existing limitations within NevarokML. It is important to understand that change requests are not intended for reporting bugs, as they require specific information for debugging.</li> </ul>"},{"location":"contributing/requesting-a-change/#source-of-inspiration","title":"Source of inspiration","text":"<ul> <li>If you have seen your idea implemented in other machine learning libraries or frameworks, gather enough information about its implementation. Explain what aspects you like and dislike about the implementation, as this helps us evaluate its potential fit.</li> </ul>"},{"location":"contributing/requesting-a-change/#benefit-for-the-community","title":"Benefit for the community","text":"<ul> <li>NevarokML aims to serve a wide range of users in the machine learning community. When evaluating new ideas, it's essential to consider the potential impact and benefit for other users. Seek input from the community and consider alternative viewpoints to ensure the proposed change benefits a larger audience.</li> </ul>"},{"location":"contributing/requesting-a-change/#issue-template","title":"Issue template","text":"<p>Once you have completed the preliminary work and ensured that your idea meets our requirements, you can proceed with creating a change request using the following guide:</p>"},{"location":"contributing/requesting-a-change/#title","title":"Title","text":"<ul> <li>Create a concise and descriptive title that summarizes your idea in one sentence. The title should reflect the potential impact and benefit for the NevarokML community.</li> </ul>"},{"location":"contributing/requesting-a-change/#context-optional","title":"Context (optional)","text":"<ul> <li>If necessary, provide additional context to help us understand your objectives and the circumstances in which you are using NevarokML. Please refrain from describing the change request in this section.</li> </ul>"},{"location":"contributing/requesting-a-change/#description","title":"Description","text":"<ul> <li>Provide a detailed and clear description of your idea. Focus on explaining why your idea is relevant to NevarokML and how it can improve the library's functionality or address existing limitations. Keep the description concise and avoid over-describing the idea. If you have multiple unrelated ideas, create separate change requests for each.</li> </ul>"},{"location":"contributing/requesting-a-change/#related-links","title":"Related links","text":"<ul> <li>If there are any relevant links to discussions, issues, or documentation sections related to your change request, please include them. This will provide additional context and help us evaluate the feedback and input already provided by the community.</li> </ul>"},{"location":"contributing/requesting-a-change/#use-cases","title":"Use cases","text":"<ul> <li>Explain how your proposed change would work from the perspective of NevarokML users. Describe the expected impact and benefits for not only yourself but also other users. Consider any potential implications on existing functionality and whether the change may break any existing features.</li> </ul>"},{"location":"contributing/requesting-a-change/#visuals-optional","title":"Visuals (optional)","text":"<ul> <li>If you have any visuals, such as diagrams, screenshots, or examples, that can help illustrate your proposed change, please include them in this section. Visuals can provide additional clarity and help maintainers better understand your idea.</li> </ul>"},{"location":"contributing/requesting-a-change/#checklist","title":"Checklist","text":"<p>Thanks for following the change request guide for NevarokML. This section ensures that you have read this guide and have provided all the necessary information for us to review your proposed change. We appreciate your contribution, and we will take it from here.</p>"},{"location":"license/","title":"License Agreements","text":"<p>Welcome to the license agreements page for our products. Here, you can find all the relevant license agreements that govern the use of products and materials. It is important to carefully read and understand these agreements before using any of our product.</p> <p>Note</p> <p>Please note that the license agreements may vary depending on the specific product you are using. To ensure compliance, refer to the corresponding license agreement for the particular product you are interested in.</p>"},{"location":"license/#license-agreements_1","title":"License Agreements","text":"<ul> <li>NevarokML License Agreement</li> </ul>"},{"location":"license/#contact-information","title":"Contact Information","text":"<ul> <li>If you have any questions or concerns regarding the license agreements or any product, please contact us</li> </ul> <p>We strive to provide clear and transparent licensing terms to ensure a positive user experience and compliance with applicable laws and regulations. Thank you for choosing us and our products.</p>"},{"location":"license/nevarok-ml-license/","title":"NevarokML License Agreement","text":"<p>This License Agreement (\"Agreement\") is entered into by and between Kyrylo Mishakin, also known as nevarok (\"Author\"), and the licensee (\"Licensee\") for the usage of the Unreal Engine plugin named \"NevarokML\" (\"Plugin\").</p> <p>Grant of License: 1.1. Author grants Licensee a non-exclusive, non-transferable license to use the Plugin      for the Licensee's internal business purposes, subject to the terms and conditions      of this Agreement. 1.2. This license is specifically granted on a per-seat basis. Each Licensee must obtain      a separate license for each individual who will use the Plugin. 1.3. The license granted herein does not include the right to sublicense, distribute,      sell, or otherwise make the Plugin available to third parties.</p> <p>Ownership and Intellectual Property: 2.1. The Plugin, including all intellectual property rights, remains the sole and exclusive      property of Kyrylo Mishakin. Licensee acknowledges and agrees that no ownership or      intellectual property rights are transferred under this Agreement.</p> <p>Restrictions: 3.1. Licensee shall not reverse engineer, decompile, disassemble, or create      derivative works based on the Plugin, except as expressly permitted by applicable law. 3.2. Licensee shall not remove, alter, or obscure any copyright, trademark, or proprietary      notices embedded in or displayed by the Plugin. 3.3. Licensee shall not use the Plugin for any illegal or unauthorized purpose, including      but not limited to infringing upon any third-party intellectual property rights.</p> <p>Support and Updates: 4.1. Author shall provide reasonable support to Licensee regarding the functionality and use      of the Plugin. However, Author is under no obligation to provide updates, bug fixes, or      new versions of the Plugin.</p> <p>Term and Termination: 5.1. This Agreement shall remain in effect unless terminated by either party. Either party may      terminate this Agreement immediately upon written notice if the other party breaches any      material provision of this Agreement. 5.2. Upon termination, Licensee shall immediately cease all use of the Plugin and delete or      destroy any copies of the Plugin in their possession or control.</p> <p>Limitation of Liability: 6.1. In no event shall Kyrylo Mishakin be liable for any indirect, incidental, consequential,      or punitive damages arising out of or in connection with the use or inability to use the      Plugin, even if Kyrylo Mishakin has been advised of the possibility of such damages.</p> <p>Governing Law and Jurisdiction: 7.1. This Agreement shall be governed by and construed in accordance with the laws of the jurisdiction      where Kyrylo Mishakin is located. 7.2. Any disputes arising out of or in connection with this Agreement shall be subject to the exclusive      jurisdiction of the courts in the jurisdiction where Kyrylo Mishakin is located.</p> <p>By using the Plugin, Licensee acknowledges that they have read and understood this Agreement and agree to be bound by its terms and conditions.</p> <p>Copyright \u00a9 2023 Kyrylo Mishakin</p>"},{"location":"nevarok-ml/","title":"NevarokML: Machine Learning for Unreal Engine","text":"<p>Welcome to NevarokML, the advanced plugin that brings the power of machine learning to Unreal Engine. NevarokML allows you to seamlessly train machine learning models right from within the Unreal Engine environment using stable-baselines3.</p> <p>With NevarokML, you have access to a wide range of supported algorithms, including PPO, A2C, DDPG, DQN, SAC, and TD3, enabling you to choose the one that best suits your needs. The plugin also supports multi-agent training and multi-environment training, offering flexibility and scalability in your machine learning endeavors.</p> <p>Train your models using NevarokML using either C++ or Blueprints, providing developers with the freedom to utilize their preferred programming approach. Whether you're an experienced coder or a visual scripting enthusiast, NevarokML supports you in creating intelligent systems within Unreal Engine.</p> <p>After training your models, NevarokML allows you to import them into Unreal Engine as NNEModelData, seamlessly integrating them into your game or simulation. These trained models can be deployed and shipped to platforms supported by Unreal's Neural Network Engine (NNE), unlocking new possibilities for intelligent and immersive experiences.</p> <p>NevarokML goes beyond gaming and simulation. It can be a powerful tool for solving game design issues and training intelligent non-player characters (NPCs). Its applications are virtually limitless, opening doors to innovation and creativity in various fields.</p> <p>Explore the endless possibilities of machine learning in Unreal Engine with NevarokML. Start leveraging its capabilities today and unlock a new level of intelligent virtual experiences.</p>"},{"location":"nevarok-ml/documentation/a2c/","title":"NevarokML: A2C Algorithm","text":"<p>The NevarokML plugin integrates the powerful Stable Baselines Advantage Actor Critic (A2C) algorithm into the Unreal Engine environment. A2C is an on-policy actor-critic algorithm that combines the benefits of value-based and policy-based methods.</p>"},{"location":"nevarok-ml/documentation/a2c/#a2c-algorithm-overview","title":"A2C Algorithm Overview","text":"<p>The A2C algorithm, as implemented in NevarokML, utilizes a policy model and value function model to estimate the advantage function. It employs advantage estimation and a discounted cumulative reward to optimize both the policy and value function simultaneously. Here are the key features and parameters of the A2C algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the A2C algorithm. It controls the step size during optimization and affects the convergence speed and stability.</li> <li>nSteps (Number of Steps): Specify the number of steps to run for each environment per update. This determines the size of the rollout buffer and influences the trade-off between bias and variance.</li> <li>gamma (Discount Factor (Gamma)): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>gaeLambda (Generalized Advantage Estimation (GAE) Lambda): Specify the trade-off factor between bias and variance for the Generalized Advantage Estimator. It affects how rewards are accumulated over time and influences the agent's value function estimation.</li> <li>entCoef (Entropy Coefficient): Set the entropy coefficient for the loss calculation. It encourages exploration by adding an entropy term to the objective function.</li> <li>vfCoef (Value Function Coefficient): Set the value function coefficient for the loss calculation. It balances the importance of the value function and the policy gradient during optimization.</li> <li>maxGradNorm (Maximum Gradient Norm): Specify the maximum value for gradient clipping. It prevents large updates that could destabilize the training process.</li> <li>rmsPropEps (RMSProp Epsilon): Set the epsilon value used in the RMSProp optimizer. It stabilizes the square root computation in the denominator of the RMSProp update.</li> <li>useRmsProp (Use RMSProp): Choose whether to use RMSProp (default) or Adam as the optimizer.</li> <li>useSde (Use SDE): Enable the use of Generalized State Dependent Exploration (gSDE) instead of action noise exploration.</li> <li>sdeSampleFreq (SDE Sample Frequency): Set the frequency to sample a new noise matrix when using gSDE.</li> <li>normalizeAdvantage (Normalize Advantage): Choose whether to normalize the advantage values during training.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/a2c/#api","title":"API","text":"<p>Here is API for A2C algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* A2C(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 7e-4,\nconst int nSteps = 5,\nconst float gamma = 0.99,\nconst float gaeLambda = 1.0,\nconst float entCoef = 0.0,\nconst float vfCoef = 0.5,\nconst float maxGradNorm = 0.5,\nconst float rmsPropEps = 1e-5,\nconst bool useRmsProp = true,\nconst bool useSde = false,\nconst int sdeSampleFreq = -1,\nconst bool normalizeAdvantage = false,\nconst int verbose = 1);\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the A2C algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines A2C algorithm, please refer to the original paper, stable-baselines3 documentation page, and the introduction to A2C guide.</p>"},{"location":"nevarok-ml/documentation/binary/","title":"NevarokML: Binary Space","text":"<p>Warning</p> <p>This documentation page is reserved for the upcoming feature: <code>Binary</code> space.</p>"},{"location":"nevarok-ml/documentation/box/","title":"NevarokML: Box Space","text":"<p>The NevarokML plugin provides a <code>Box</code> space implementation, representing a (possibly unbounded) box in R^n. It is commonly used to define the observation space in reinforcement learning environments where the state can have continuous values.</p>"},{"location":"nevarok-ml/documentation/box/#box-space-overview","title":"Box Space Overview","text":"<p>The <code>Box</code> space in NevarokML represents a Cartesian product of n closed intervals in R^n. It can have either an identical bound for each dimension or independent bounds for each dimension. Here are the key features and parameters of the <code>Box</code> space:</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the box space.</li> <li>min (Low): The lower bound of the box, specifying the minimum value for each dimension.</li> <li>max (High): The upper bound of the box, specifying the maximum value for each dimension.</li> </ul>"},{"location":"nevarok-ml/documentation/box/#api","title":"API","text":"<p>Here is the API for the <code>Box</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Box(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max)\n</code></pre> <p>To create a <code>Box</code> space, call the <code>Box</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>Box</code> space.</p>"},{"location":"nevarok-ml/documentation/boxdxstack/","title":"NevarokML: BoxDXStack Space","text":"<p>The NevarokML plugin provides a <code>BoxDXStack</code> space implementation, which represents a stack of n-dimensional continuous boxes. This space is useful for tasks where the agent needs to maintain a memory of continuous box values.</p>"},{"location":"nevarok-ml/documentation/boxdxstack/#boxdxstack-space-overview","title":"BoxDXStack Space Overview","text":"<p>The <code>BoxDXStack</code> space in NevarokML extends the functionality of the Box space by adding a stack dimension. It allows you to stack multiple instances of the n-dimensional continuous boxes, creating a memory of continuous box values.</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the box space.</li> <li>min (Low): The lower bound of the box, specifying the minimum value for each dimension.</li> <li>max (High): The upper bound of the box, specifying the maximum value for each dimension.</li> <li>stack: The size of the stack. Specifies how many instances of the n-dimensional continuous boxes are stacked.</li> </ul> <p>Note</p> <p>The values are stacked along the X-axis.</p>"},{"location":"nevarok-ml/documentation/boxdxstack/#api","title":"API","text":"<p>Here is the API for the <code>BoxDXStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* BoxDXStack(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> <p>To create a <code>BoxDXStack</code> space, call the <code>BoxDXStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>BoxDXStack</code> space.</p>"},{"location":"nevarok-ml/documentation/boxdystack/","title":"NevarokML: BoxDYStack Space","text":"<p>The NevarokML plugin provides a <code>BoxDYStack</code> space implementation, which represents a stack of n-dimensional continuous boxes. This space is useful for tasks where the agent needs to maintain a memory of continuous box values.</p>"},{"location":"nevarok-ml/documentation/boxdystack/#boxdystack-space-overview","title":"BoxDYStack Space Overview","text":"<p>The <code>BoxDYStack</code> space in NevarokML extends the functionality of the Box space by adding a stack dimension. It allows you to stack multiple instances of the n-dimensional continuous boxes, creating a memory of continuous box values.</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the box space.</li> <li>min (Low): The lower bound of the box, specifying the minimum value for each dimension.</li> <li>max (High): The upper bound of the box, specifying the maximum value for each dimension.</li> <li>stack: The size of the stack. Specifies how many instances of the n-dimensional continuous boxes are stacked.</li> </ul> <p>Note</p> <p>The values are stacked along the Y-axis.</p>"},{"location":"nevarok-ml/documentation/boxdystack/#api","title":"API","text":"<p>Here is the API for the <code>BoxDYStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* BoxDYStack(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> <p>To create a <code>BoxDYStack</code> space, call the <code>BoxDYStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>BoxDYStack</code> space.</p>"},{"location":"nevarok-ml/documentation/continuous/","title":"NevarokML: Continuous Space","text":"<p>The NevarokML plugin provides a <code>Continuous</code> space implementation, representing a single-dimensional <code>Box</code> space in Stable Baselines3.</p>"},{"location":"nevarok-ml/documentation/continuous/#continuous-space-overview","title":"Continuous Space Overview","text":"<p>The <code>Continuous</code> space in NevarokML corresponds to the <code>Box</code> space in Stable Baselines3. It represents a single-dimensional box in R. The interval can have the form of [a, b] allowing for a wide range of continuous values.</p> <ul> <li>owner: The owner object of the space (usually the object creating the space).</li> <li>size (Number of Elements): The size of the continuous array.</li> <li>min: An array of minimum values specifying the lower bounds for each element.</li> <li>max: An array of maximum values specifying the upper bounds for each element.</li> </ul>"},{"location":"nevarok-ml/documentation/continuous/#api","title":"API","text":"<p>Here is the API for the <code>Continuous</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Continuous(UObject* owner, int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max)\n</code></pre> <p>To create a <code>Continuous</code> space, call the <code>Continuous</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>Continuous</code> space.</p>"},{"location":"nevarok-ml/documentation/continuousstack/","title":"NevarokML: ContinuousStack Space","text":"<p>The NevarokML plugin provides a <code>ContinuousStack</code> space implementation, which represents a stack of continuous values. This space is useful for tasks where the agent needs to maintain a memory of continuous values.</p>"},{"location":"nevarok-ml/documentation/continuousstack/#continuousstack-space-overview","title":"ContinuousStack Space Overview","text":"<p>The <code>ContinuousStack</code> space in NevarokML extends the functionality of the Continuous space by adding a stack dimension. It allows you to stack multiple instances of the continuous spaces values, creating a memory of continuous values.</p> <ul> <li>owner: Represents the owner of the space object, usually the object creating the space.</li> <li>size: The size of the continuous space.</li> <li>min: An array of minimum values for each dimension of the continuous space.</li> <li>max: An array of maximum values for each dimension of the continuous space.</li> <li>stack: The size of the stack. Specifies how many instances of the continuous action space are stacked.</li> </ul>"},{"location":"nevarok-ml/documentation/continuousstack/#api","title":"API","text":"<p>Here is the API for the <code>ContinuousStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* ContinuousStack(UObject* owner, int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> <p>To create a <code>ContinuousStack</code> space, call the <code>ContinuousStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>ContinuousStack</code> space.</p>"},{"location":"nevarok-ml/documentation/ddpg/","title":"NevarokML: DDPG (TD3) Algorithm","text":"<p>The NevarokML plugin integrates the Deep Deterministic Policy Gradient (DDPG) algorithm, which is a special case of its successor Twin Delayed DDPG (TD3), into the Unreal Engine environment. DDPG is an off-policy actor-critic algorithm that can handle continuous action spaces. It combines the benefits of the deterministic policy gradient algorithm and the Q-learning algorithm.</p>"},{"location":"nevarok-ml/documentation/ddpg/#ddpg-algorithm-overview","title":"DDPG Algorithm Overview","text":"<p>The DDPG algorithm, as implemented in NevarokML, consists of an actor-critic architecture where the actor is a deterministic policy and the critic estimates the Q-value function. It uses a replay buffer to store and sample experiences for training. Here are the key features and parameters of the DDPG algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/ddpg/#api","title":"API","text":"<p>Here is the API for the DDPG (TD3) algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* DDPG(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-3,\nconst int bufferSize = 1000000,\nconst int learningStarts = 100,\nconst int batchSize = 100,\nconst float tau = 0.005,\nconst float gamma = 0.99,\nconst int trainFreq = 1,\nconst int gradientSteps = -1,\nconst bool optimizeMemoryUsage = false,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the DDPG (TD3) algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines DDPG algorithm, please refer to the Deterministic Policy Gradient paper, DDPG Paper, stable-baselines3 documentation page, and the introduction to DDPG guide.</p>"},{"location":"nevarok-ml/documentation/discrete/","title":"NevarokML: Discrete Space","text":"<p>The NevarokML plugin provides a <code>Discrete</code> space implementation, representing a discrete space with values in the set <code>{0, 1, ..., n-1}</code>. It is commonly used to define the action space in reinforcement learning environments where the agent can choose from a finite number of actions.</p>"},{"location":"nevarok-ml/documentation/discrete/#discrete-space-overview","title":"Discrete Space Overview","text":"<p>The <code>Discrete</code> space in NevarokML corresponds to a discrete set of values ranging from 0 to n-1. It is used to represent the action space in reinforcement learning environments. Here's the key feature and parameter of the <code>Discrete</code> space:</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (N): The size of the discrete space, indicating the number of possible values the space can take.</li> </ul>"},{"location":"nevarok-ml/documentation/discrete/#api","title":"API","text":"<p>Here is the API for the <code>Discrete</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Discrete(UObject* owner, int64 size = 1)\n</code></pre> <p>To create a <code>Discrete</code> space, call the <code>Discrete</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>Discrete</code> space.</p>"},{"location":"nevarok-ml/documentation/dqn/","title":"NevarokML: DQN Algorithm","text":"<p>The NevarokML plugin integrates the Deep Q-Network (DQN) algorithm into the Unreal Engine environment. DQN is an off-policy algorithm that combines reinforcement learning with deep neural networks to solve complex tasks. It is based on the original DQN paper and subsequent improvements.</p>"},{"location":"nevarok-ml/documentation/dqn/#dqn-algorithm-overview","title":"DQN Algorithm Overview","text":"<p>The DQN algorithm, as implemented in NevarokML, uses a combination of a deep neural network and a replay buffer to approximate the optimal action-value function. It employs techniques such as experience replay and target networks to stabilize the learning process. Here are the key features and parameters of the DQN algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>targetUpdateInterval (Target Update Interval): Specify the interval to update the target network. It determines how often the target network is synchronized with the online network.</li> <li>explorationFraction (Exploration Fraction): Set the fraction of the entire training period over which the exploration rate is reduced. It controls the balance between exploration and exploitation.</li> <li>explorationInitialEps (Exploration Initial Epsilon): Set the initial value of the random action probability. It determines the exploration rate at the beginning of the training.</li> <li>explorationFinalEps (Exploration Final Epsilon): Set the final value of the random action probability. It determines the exploration rate at the end of the training.</li> <li>maxGradNorm (Maximum Gradient Norm): Specify the maximum value for gradient clipping. It prevents large updates that could destabilize the training process.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/dqn/#api","title":"API","text":"<p>Here is the API for the DQN algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* DQN(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-4,\nconst int bufferSize = 1000000,\nconst int learningStarts = 50000,\nconst int batchSize = 32,\nconst float tau = 1.0,\nconst float gamma = 0.99,\nconst int trainFreq = 4,\nconst int gradientSteps = 1,\nconst bool optimizeMemoryUsage = false,\nconst int targetUpdateInterval = 10000,\nconst float explorationFraction = 0.1,\nconst float explorationInitialEps = 1.0,\nconst float explorationFinalEps = 0.05,\nconst float maxGradNorm = 10,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the DQN algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines DQN algorithm, please refer to the DQN Paper, stable-baselines3 documentation page, and the Nature paper.</p>"},{"location":"nevarok-ml/documentation/multibinary/","title":"NevarokML: MultiBinary Space","text":"<p>The NevarokML plugin provides a <code>MultiBinary</code> space implementation, representing a single-dimensional <code>MultiBinary</code> space in Stable Baselines3.</p>"},{"location":"nevarok-ml/documentation/multibinary/#multibinary-space-overview","title":"MultiBinary Space Overview","text":"<p>The <code>MultiBinary</code> space in NevarokML corresponds to the <code>MultiBinary</code> space in Stable Baselines3. It represents an n-shape binary space, where the argument <code>n</code> defines the number of elements in the space.</p> <ul> <li>owner: The owner object of the space (usually the object creating the space).</li> <li>size (Number of Elements): The size of the multi binary space.</li> </ul>"},{"location":"nevarok-ml/documentation/multibinary/#multibinary-space-creation","title":"MultiBinary Space Creation","text":"<p>To create a <code>MultiBinary</code> space in NevarokML, use the following factory function:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinary(UObject* owner, int32 size = 1)\n</code></pre> <p>To create a <code>MultiBinary</code> space, call the <code>MultiBinary</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiBinary</code> space.</p>"},{"location":"nevarok-ml/documentation/multibinary_hidden/","title":"NevarokML: MultiBinary Space","text":"<p>The NevarokML plugin provides a <code>MultiBinary</code> space implementation, representing a multidimensional binary space. It is commonly used to define the observation space in reinforcement learning environments where the state consists of multiple binary values.</p>"},{"location":"nevarok-ml/documentation/multibinary_hidden/#multibinary-space-overview","title":"MultiBinary Space Overview","text":"<p>The <code>MultiBinary</code> space in NevarokML represents a multidimensional binary space, where each element can take a binary value (0 or 1).</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the multi binary space.</li> </ul>"},{"location":"nevarok-ml/documentation/multibinary_hidden/#api","title":"API","text":"<p>Here is the API for the <code>MultiBinary</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinary(UObject* owner, FNevarokMLIndex2D size)\n</code></pre> <p>To create a <code>MultiBinary</code> space, call the <code>MultiBinary</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiBinary</code> space.</p>"},{"location":"nevarok-ml/documentation/multibinarystack/","title":"NevarokML: MultiBinaryStack Space","text":"<p>The NevarokML plugin provides a <code>MultiBinaryStack</code> space implementation, which represents a stack of multi-binary values. This space is useful for tasks where the agent needs to maintain a memory of binary values.</p>"},{"location":"nevarok-ml/documentation/multibinarystack/#multibinarystack-space-overview","title":"MultiBinaryStack Space Overview","text":"<p>The <code>MultiBinaryStack</code> space in NevarokML extends the functionality of the MultiBinary space by adding a stack dimension. It allows you to stack multiple instances of the multi-binary space values, creating a memory of binary values.</p> <ul> <li>owner: Represents the owner of the space object, usually the object creating the space.</li> <li>size: The size of the binary space.</li> <li>stack: The size of the stack. Specifies how many instances of the multi-binary action space are stacked.</li> </ul>"},{"location":"nevarok-ml/documentation/multibinarystack/#api","title":"API","text":"<p>Here is the API for the <code>MultiBinaryStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinaryStack(UObject* owner, int32 size = 1, int stack = 1);\n</code></pre> <p>To create a <code>MultiBinaryStack</code> space, call the <code>MultiBinaryStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiBinaryStack</code> space.</p>"},{"location":"nevarok-ml/documentation/multidiscrete/","title":"NevarokML: MultiDiscrete Space","text":"<p>The NevarokML plugin provides a <code>MultiDiscrete</code> space implementation, representing a series of discrete action spaces with different numbers of actions in each.</p>"},{"location":"nevarok-ml/documentation/multidiscrete/#multidiscrete-space-overview","title":"MultiDiscrete Space Overview","text":"<p>The <code>MultiDiscrete</code> space in NevarokML is useful for representing game controllers or keyboards where each key can be represented as a discrete action space. It is parametrized by passing an array of positive integers specifying the number of actions for each discrete action space.</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>vec: An array of positive integers specifying the number of actions for each discrete action space.</li> </ul>"},{"location":"nevarok-ml/documentation/multidiscrete/#api","title":"API","text":"<p>Here is the API for the <code>MultiDiscrete</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiDiscrete(UObject* owner, const TArray&lt;int64&gt;&amp; vec)\n</code></pre> <p>To create a <code>MultiDiscrete</code> space, call the <code>MultiDiscrete</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiDiscrete</code> space.</p>"},{"location":"nevarok-ml/documentation/multidiscretestack/","title":"NevarokML: MultiDiscreteStack Space","text":"<p>The NevarokML plugin provides a <code>MultiDiscreteStack</code> space implementation, which represents a stack of multi-discrete action values. This space is useful for tasks where the agent needs to maintain a memory.</p>"},{"location":"nevarok-ml/documentation/multidiscretestack/#multidiscretestack-space-overview","title":"MultiDiscreteStack Space Overview","text":"<p>The <code>MultiDiscreteStack</code> space in NevarokML extends the functionality of the MultiDiscrete space by adding a stack dimension. It allows you to stack multiple instances of the multi-discrete space values, creating a memory of multi-discrete values.</p> <ul> <li>owner: Represents the owner of the space object, usually the object creating the space.</li> <li>vec: An array of positive integers specifying the number of actions for each discrete action space.</li> <li>stack: The size of the stack. Specifies how many values of the multi-discrete action space are stacked.</li> </ul>"},{"location":"nevarok-ml/documentation/multidiscretestack/#api","title":"API","text":"<p>Here is the API for the <code>MultiDiscreteStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiDiscreteStack(UObject* owner, const TArray&lt;int64&gt;&amp; vec, int stack = 1);\n</code></pre> <p>To create a <code>MultiDiscreteStack</code> space, call the <code>MultiDiscreteStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiDiscreteStack</code> space.</p>"},{"location":"nevarok-ml/documentation/ppo/","title":"NevarokML: PPO Algorithm","text":"<p>The NevarokML plugin integrates the powerful Stable Baselines Proximal Policy Optimization (PPO) algorithm into the Unreal Engine environment. PPO is a state-of-the-art reinforcement learning algorithm that has demonstrated excellent performance in a wide range of applications.</p>"},{"location":"nevarok-ml/documentation/ppo/#ppo-algorithm-overview","title":"PPO Algorithm Overview","text":"<p>The PPO algorithm, as implemented in NevarokML, combines the benefits of policy gradient methods with an iterative optimization approach. It leverages a clipped surrogate objective function to ensure stable and efficient training. Here are the key features and parameters of the PPO algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and   learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the PPO algorithm. It controls the step size during   optimization and affects the convergence speed and stability.</li> <li>nSteps (Number of Steps): Specify the number of steps to run for each environment per update. This determines the   size of the rollout buffer and influences the trade-off between bias and variance.</li> <li>batchSize (Batch Size): Set the minibatch size used for training. Larger batch sizes can improve training   stability but require more computational resources.</li> <li>nEpochs (Number of Epochs): Define the number of epochs when optimizing the surrogate loss. Each epoch consists of   multiple update steps on the collected samples.</li> <li>gamma (Discount Factor (Gamma)): Set the discount factor that determines the weight of future rewards compared to   immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>gaeLambda (Generalized Advantage Estimation (GAE) Lambda): Specify the trade-off factor between bias and variance   for the Generalized Advantage Estimator. It affects how rewards are accumulated over time and influences the agent's   value function estimation.</li> <li>clipRange (Clip Range): Set the clipping parameter for the PPO objective. It restricts the policy update to a   certain range, preventing drastic policy changes.</li> <li>entCoef (Entropy Coefficient): Specify the entropy coefficient for the loss calculation. It encourages exploration   by adding an entropy term to the objective function.</li> <li>vfCoef (Value Function Coefficient): Set the value function coefficient for the loss calculation. It balances the   importance of the value function and the policy gradient during optimization.</li> <li>maxGradNorm (Maximum Gradient Norm): Specify the maximum value for gradient clipping. It prevents large updates   that could destabilize the training process.</li> <li>useSde (Use SDE): Enable the use of Generalized State Dependent Exploration (gSDE) instead of action noise   exploration.</li> <li>sdeSampleFreq (SDE Sample Frequency): Set the frequency to sample a new noise matrix when using gSDE.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for   info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/ppo/#api","title":"API","text":"<p>Here is API for PPO algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* PPO(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 3e-4,\nconst int nSteps = 2048,\nconst int batchSize = 64,\nconst int nEpochs = 10,\nconst float gamma = 0.99,\nconst float gaeLambda = 0.95,\nconst float clipRange = 0.2,\nconst float entCoef = 0.0,\nconst float vfCoef = 0.5,\nconst float maxGradNorm = 0.5,\nconst bool useSde = false,\nconst int sdeSampleFreq = -1,\nconst int verbose = 1);\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the PPO algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines PPO algorithm, please refer to the original paper,  stable-baselines3 documentation page and the Spinning Up guide.</p>"},{"location":"nevarok-ml/documentation/sac/","title":"NevarokML: SAC Algorithm","text":"<p>The NevarokML plugin integrates the Soft Actor-Critic (SAC) algorithm into the Unreal Engine environment. SAC is an off-policy maximum entropy deep reinforcement learning algorithm that combines actor-critic methods with entropy regularization to achieve both exploration and exploitation in learning. It is based on the original SAC paper and subsequent improvements.</p>"},{"location":"nevarok-ml/documentation/sac/#sac-algorithm-overview","title":"SAC Algorithm Overview","text":"<p>The SAC algorithm, as implemented in NevarokML, utilizes a stochastic actor-critic architecture with double Q-targets to estimate the action-value function. It incorporates entropy regularization to encourage exploration and maximize the entropy of the policy distribution. Here are the key features and parameters of the SAC algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>entCoefAuto: Set the entropy regularization coefficient. Set it to 'auto' to learn it automatically.</li> <li>entCoef (Entropy Coefficient): Set the entropy regularization coefficient. It controls the trade-off between exploration and exploitation. Value less or equal to '0.0' will set 'entCoef' to 'auto' to learn it automatically, value greater '0.0' will set 'entCoef' to 'auto_{entCoef} if 'entCoefAuto' is set to 'true''.</li> <li>targetUpdateInterval (Target Update Interval): Specify the interval to update the target network. It determines how often the target network is synchronized with the online network.</li> <li>targetEntropyAuto: Set the target entropy when learning the entropy coefficient. Set it to 'auto' to learn it automatically.</li> <li>targetEntropy (Target Entropy): Set the target entropy when learning the entropy coefficient. Ignored if 'targetEntropyAuto' is set to 'true'. Value less or equal to '0.0' will set 'targetEntropy' to 'auto'.</li> <li>useSde (Use Generalized State Dependent Exploration): Enable the use of generalized State Dependent Exploration (gSDE) instead of action noise exploration.</li> <li>sdeSampleFreq (SDE Sample Frequency): Set the frequency to sample a new noise matrix when using gSDE. Set to -1 to sample only at the beginning of the rollout.</li> <li>useSdeAtWarmup (Use SDE at Warmup): Specify whether to use gSDE instead of uniform sampling during the warm-up phase before learning starts.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/sac/#api","title":"API","text":"<p>Here is the API for the SAC algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* SAC(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 3e-4,\nconst int bufferSize = 1000000,\nconst int learningStarts = 100,\nconst int batchSize = 256,\nconst float tau = 0.005,\nconst float gamma = 0.99,\nconst int trainFreq = 1,\nconst int gradientSteps = 1,\nconst bool optimizeMemoryUsage = false,\nconst bool entCoefAuto = true,\nconst float entCoef = 0.0,\nconst int targetUpdateInterval = 1,\nconst bool targetEntropyAuto = true,\nconst float targetEntropy = 0.0,\nconst bool useSde = false,\nconst int sdeSampleFreq = -1,\nconst bool useSdeAtWarmup = false,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the SAC algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines SAC algorithm, please refer to the SAC Paper, stable-baselines3 documentation page and the OpenAI Spinning Up Introduction to SAC.</p>"},{"location":"nevarok-ml/documentation/td3/","title":"NevarokML: TD3 Algorithm","text":"<p>The NevarokML plugin integrates the Twin Delayed DDPG (TD3) algorithm into the Unreal Engine environment. TD3 is an off-policy actor-critic algorithm that addresses function approximation errors in traditional actor-critic methods. It combines insights from the Deep Deterministic Policy Gradient (DDPG) and Twin Delayed DDPG papers to improve stability and performance.</p>"},{"location":"nevarok-ml/documentation/td3/#td3-algorithm-overview","title":"TD3 Algorithm Overview","text":"<p>The TD3 algorithm, as implemented in NevarokML, utilizes a twin critic architecture and delayed policy updates to improve the learning process. It maintains two Q-value networks to reduce overestimation bias. Here are the key features and parameters of the TD3 algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>policyDelay (Policy Delay): Set the number of steps between policy updates. The policy and target networks will only be updated once every policy_delay steps per training step.</li> <li>targetPolicyNoise (Target Policy Noise): Set the standard deviation of Gaussian noise added to the target policy (smoothing noise).</li> <li>targetNoiseClip (Target Noise Clip): Set the limit for the absolute value of the target policy smoothing noise.</li> <li>verbose (Verbosity Level): Set the verbosity level for the algorithm's output. Use 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/td3/#api","title":"API","text":"<p>Here is the API for the TD3 algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* TD3(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-3,\nconst int bufferSize = 1000000,\nconst int learningStarts = 100,\nconst int batchSize = 100,\nconst float tau = 0.005,\nconst float gamma = 0.99,\nconst int trainFreq = 1,\nconst int gradientSteps = -1,\nconst bool optimizeMemoryUsage = false,\nconst int policyDelay = 2,\nconst float targetPolicyNoise = 0.2,\nconst float targetNoiseClip = 0.5,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the TD3 algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines TD3 algorithm, please refer to the TD3 Paper, stable-baselines3 documentation page and the OpenAI Spinning Up Introduction to TD3.</p>"},{"location":"nevarok-ml/examples/","title":"Getting Started with Reinforcement Learning and NevarokML","text":"<p>To start exploring reinforcement learning with NevarokML, here are some steps you can follow:</p> <ul> <li> <p>Familiarize Yourself: Gain a solid understanding of reinforcement learning concepts, algorithms, and frameworks. Learn about Markov Decision Processes (MDPs), value functions, policy gradients, and deep reinforcement learning techniques.</p> </li> <li> <p>Install NevarokML: Follow the installation instructions for NevarokML and set up the plugin within your Unreal Engine project.</p> </li> <li> <p>Learn the Basics: Dive into introductory tutorials and resources on reinforcement learning. Understand the fundamentals of training agents, defining reward structures, and implementing reinforcement learning algorithms.</p> </li> <li> <p>Experiment with Environments: Create simple environments within Unreal Engine to train and test your reinforcement learning agents. Start with basic tasks and gradually increase the complexity of the environments to challenge your agents.</p> </li> <li> <p>Work with NevarokML Examples: Explore the example projects and tutorials provided with NevarokML. Gain hands-on experience by working through these examples and understanding how reinforcement learning can be applied in different scenarios.</p> </li> <li> <p>Extend Your Knowledge: Explore advanced topics in reinforcement learning, such as hierarchical reinforcement learning, multi-agent reinforcement learning, and imitation learning. Stay updated with the latest research and advancements in the field.</p> </li> <li> <p>Join the Community: Engage with the reinforcement learning and NevarokML communities. Participate in forums, online communities, and social media platforms to discuss ideas, share insights, and learn from others' experiences.</p> </li> </ul> <p>Reinforcement learning combined with NevarokML brings unprecedented opportunities for creating intelligent and adaptive systems within Unreal Engine. Embrace the challenges and rewards of reinforcement learning, and join us in shaping the future of machine learning in the world of game development.</p>"},{"location":"nevarok-ml/getting-started/","title":"NevarokML: Getting Started with Reinforcement Learning","text":"<p>To start exploring reinforcement learning with NevarokML, here are some steps you can follow:</p> <ul> <li> <p>Familiarize Yourself: Gain a solid understanding of reinforcement learning concepts, algorithms, and frameworks. Learn about Markov Decision Processes (MDPs), value functions, policy gradients, and deep reinforcement learning techniques.</p> </li> <li> <p>Install NevarokML: Follow the installation instructions for NevarokML and set up the plugin within your Unreal Engine project.</p> </li> <li> <p>Learn the Basics: Dive into introductory tutorials and resources on reinforcement learning. Understand the fundamentals of training agents, defining reward structures, and implementing reinforcement learning algorithms.</p> </li> <li> <p>Experiment with Environments: Create simple environments within Unreal Engine to train and test your reinforcement learning agents. Start with basic tasks and gradually increase the complexity of the environments to challenge your agents.</p> </li> <li> <p>Work with NevarokML Examples: Explore the example projects and tutorials provided with NevarokML. Gain hands-on experience by working through these examples and understanding how reinforcement learning can be applied in different scenarios.</p> </li> <li> <p>Extend Your Knowledge: Explore advanced topics in reinforcement learning, such as hierarchical reinforcement learning, multi-agent reinforcement learning, and imitation learning. Stay updated with the latest research and advancements in the field.</p> </li> <li> <p>Join the Community: Engage with the reinforcement learning and NevarokML communities. Participate in forums, online communities, and social media platforms to discuss ideas, share insights, and learn from others' experiences.</p> </li> </ul> <p>Reinforcement learning combined with NevarokML brings unprecedented opportunities for creating intelligent and adaptive systems within Unreal Engine. Embrace the challenges and rewards of reinforcement learning, and join us in shaping the future of machine learning in the world of game development.</p>"},{"location":"nevarok-ml/getting-started/installation/","title":"NevarokML: Installing Plugin","text":""},{"location":"nevarok-ml/getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing NevarokML, ensure that you have the following prerequisites:</p> <ul> <li>Unreal Engine v5.2 or later: NevarokML requires Unreal Engine version 5.2 or higher. If you don't have Unreal   Engine   installed, visit the official Unreal Engine website, create an Epic Games account, and download the desired version   from   the available options.</li> </ul> <p>Warning</p> <p>Make sure to install Unreal Engine before proceeding with NevarokML installation. Having Unreal Engine v5.2 or later installed on your computer is essential to ensure compatibility and proper functioning of the NevarokML plugin. Once you have met this prerequisite, you can proceed with the installation process.</p> <p>Note</p> <p>Keep in mind that NevarokML is a commercial plugin, and you need to purchase it from the Unreal Engine Marketplace before you can install and use it in your projects.</p>"},{"location":"nevarok-ml/getting-started/installation/#installation-guide","title":"Installation guide","text":"<p>To install NevarokML, a powerful reinforcement learning plugin for Unreal Engine, follow these steps:</p>"},{"location":"nevarok-ml/getting-started/installation/#step-1-access-unreal-engine-marketplace","title":"Step 1: Access Unreal Engine Marketplace","text":"<ul> <li>Go to the Unreal Engine Marketplace website and log in to your Epic Games   account. Navigate to the NevarokML page.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-2-purchase-nevarokml","title":"Step 2: Purchase NevarokML","text":"<ul> <li>Click on the \"Buy\" or \"Add to Cart\" button to purchase NevarokML. Complete the payment process as directed. NevarokML   is   a paid plugin, so make sure you have the necessary funds in your account.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-3-download-nevarokml","title":"Step 3: Download NevarokML","text":"<ul> <li>After the purchase is complete, go to your Epic Games Launcher. Under the \"Library\" section, navigate to the \"   Marketplace\" tab. Find NevarokML in your list of purchased items and click on the \"Install\" button to download the   plugin.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-4-install-nevarokml-in-unreal-engine","title":"Step 4: Install NevarokML in Unreal Engine","text":"<ul> <li>Open Unreal Engine on your computer. Create a new project or open an existing one. In the Unreal Engine editor, go to   the \"Edit\" menu and select \"Plugins.\" In the Plugins window, navigate to the \"Installed\" section and locate NevarokML.   Ensure that the plugin is enabled by checking the box next to its name.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-5-verify-backend-setup","title":"Step 5: Verify Backend Setup","text":"<p>To ensure that the backend for NevarokML is properly set up, follow these additional steps:</p> <ul> <li>In the Unreal Engine editor, go to the \"Edit\" menu and select \"Editor Preferences.\"</li> <li>In the Editor Preferences window, navigate to the \"Plugins\" tab and find NevarokML in the list.</li> <li>Click on NevarokML to expand its settings.</li> <li>Under the NevarokML settings, locate the \"Installation\" category.</li> <li>Verify that both options show \"OK: Backend Archive Found\" and \"OK: Backend Executable Found\" messages. If you see   any \"Error\" messages, continue to the next step.</li> </ul> <p> </p> Backend Archive Found and Backend Executable Found <ul> <li>If you see an \"Error: Archive Not Found\" message, try reimporting the package. Make sure the archive file is located   within the NevarokML plugin folder, specifically in /NevarokML/Source. After reimporting, check if   the \"OK\" messages appear. <p> </p> Error: Archive Not Found <ul> <li>If the archive is found but you see an \"Error: Backend Executable Not Found\" message, press the \"Unpack Backend\"   button. This will initiate an automated unpacking process for the backend. Wait for the process to finish.</li> </ul> <p> </p> Error: Backend Not Found <p> </p> Archive Unpacking Process <ul> <li>Once the backend setup is complete and both \"Backend Archive\" and \"Backend Executable\" show \"OK\" messages, you can   proceed to use NevarokML in your Unreal Engine projects.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-6-start-using-nevarokml","title":"Step 6: Start Using NevarokML","text":"<ul> <li>With NevarokML successfully installed, you can now start utilizing its powerful reinforcement learning capabilities   within your Unreal Engine projects. Refer to the NevarokML documentation, tutorials, and examples to learn how to   train   intelligent agents, optimize decision-making, and import pre-trained models.</li> </ul> <p>Note</p> <p>For further support or assistance, you can visit the NevarokML Contacts page. There, you will find information on how to reach out to the NevarokML team for any questions, issues, or additional guidance you may require.</p> <p>By following these steps, you can install NevarokML from the Unreal Engine Marketplace and begin incorporating reinforcement learning into your Unreal Engine projects. Enjoy exploring the possibilities of training intelligent agents and creating dynamic and adaptive virtual experiences.</p>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/","title":"Markov Decision Processes (MDPs) in Reinforcement Learning","text":"<p>Markov Decision Processes (MDPs) are fundamental mathematical models used in reinforcement learning to formalize decision-making problems. MDPs provide a framework for understanding and solving sequential decision-making tasks in uncertain environments.</p>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>States: MDPs consist of a set of states representing different configurations or conditions of the environment.</p> </li> <li> <p>Actions: Agents can take actions in each state, influencing the subsequent state transitions.</p> </li> <li> <p>Rewards: MDPs assign rewards to agents based on their actions and resulting states. The goal is to maximize the cumulative rewards over time.</p> </li> <li> <p>Transition Probabilities: MDPs define transition probabilities that determine the likelihood of transitioning from one state to another based on the chosen action.</p> </li> <li> <p>Policy: A policy defines the agent's decision-making strategy by specifying the action to take in each state.</p> </li> </ul>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/#solving-mdps","title":"Solving MDPs","text":"<p>The objective in solving MDPs is to find an optimal policy that maximizes the expected cumulative reward over time. Several algorithms and techniques can be applied to solve MDPs, including:</p> <ul> <li> <p>Value Iteration: An iterative algorithm that updates the values of states until convergence to find the optimal value function.</p> </li> <li> <p>Policy Iteration: An iterative algorithm that alternates between policy evaluation and policy improvement steps to converge on an optimal policy.</p> </li> <li> <p>Q-Learning: A model-free algorithm that learns the optimal action-value function by interacting with the environment and updating Q-values.</p> </li> <li> <p>Monte Carlo Methods: Sample-based methods that estimate value functions by averaging returns from sample trajectories.</p> </li> </ul>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/#applications-of-mdps","title":"Applications of MDPs","text":"<p>MDPs have various applications in reinforcement learning and beyond, including</p> <ul> <li> <p>Robotics: MDPs enable robots to make intelligent decisions and navigate complex environments.</p> </li> <li> <p>Game AI: MDPs form the basis for developing intelligent agents in video games, capable of adaptive and strategic behavior.</p> </li> <li> <p>Resource Management: MDPs help optimize resource allocation and scheduling in various domains, such as energy management and transportation.</p> </li> <li> <p>Finance: MDPs are used to model and optimize investment and portfolio management strategies.</p> </li> </ul> <p>In conclusion, Markov Decision Processes (MDPs) provide a formal framework for modeling and solving decision-making problems in reinforcement learning. By understanding the concepts and algorithms associated with MDPs, developers can design intelligent agents that make optimal decisions in uncertain environments.</p>"},{"location":"nevarok-ml/getting-started/reinforcement-learning/","title":"What is Reinforcement Learning","text":"<p>Reinforcement Learning is a powerful branch of machine learning that focuses on training intelligent agents to make sequential decisions in an environment to maximize a cumulative reward. It is widely used in various domains, including robotics, game playing, recommendation systems, and autonomous vehicles. With its ability to learn from interactions and adapt to dynamic environments, reinforcement learning has proven to be an effective approach for creating intelligent and adaptive systems.</p>"},{"location":"nevarok-ml/getting-started/reinforcement-learning/#how-reinforcement-learning-works","title":"How Reinforcement Learning Works","text":"<p>Reinforcement learning revolves around an agent interacting with an environment. The agent takes actions based on its current state, and the environment responds by providing feedback in the form of rewards or penalties. The goal of the agent is to learn an optimal policy\u2014a mapping from states to actions\u2014that maximizes the long-term cumulative reward.</p> <p>The reinforcement learning process typically involves the following key components:</p> <ul> <li> <p>Agent: The entity that learns and takes actions in the environment. The agent's objective is to maximize the   cumulative   reward it receives.</p> </li> <li> <p>Environment: The external system with which the agent interacts. It provides the agent with feedback in the form   of   rewards or penalties based on the agent's actions.</p> </li> <li> <p>State: The current representation of the environment that the agent perceives. The state can be a complete   snapshot of   the environment or a partial observation.</p> </li> <li> <p>Action: The decision made by the agent based on its current state. The action can have short-term consequences and   impact future states and rewards.</p> </li> <li> <p>Reward: The feedback signal provided by the environment to the agent. The reward indicates the desirability or   quality   of an action taken by the agent in a given state.</p> </li> </ul> <pre><code>sequenceDiagram\n    autonumber\n    loop until Termination Condition\n        Agent-&gt;&gt;Environment: Take Action\n        Environment--&gt;&gt;Agent: Provide Feedback (Rewards/Penalties)\n        Agent-&gt;&gt;Agent: Update Policy\n    end\n    Agent--&gt;&gt;Agent: Maximize Cumulative Reward</code></pre> <p>The reinforcement learning process involves the agent learning from trial and error. By exploring different actions and observing their consequences, the agent adjusts its policy to maximize the expected cumulative reward.</p>"},{"location":"nevarok-ml/getting-started/reinforcement-learning/#applications-of-reinforcement-learning","title":"Applications of Reinforcement Learning","text":"<p>Reinforcement learning has a wide range of applications across various domains. Some notable examples include:</p> <ul> <li> <p>Game Playing: Reinforcement learning has achieved remarkable success in mastering complex games such as Go, Chess,   and   video games. Agents trained through reinforcement learning have surpassed human-level performance in these domains.</p> </li> <li> <p>Robotics: Reinforcement learning enables robots to learn complex tasks and adapt to dynamic environments. It has   been   used in robot locomotion, manipulation, and autonomous navigation.</p> </li> <li> <p>Recommendation Systems: Reinforcement learning techniques have been employed in recommendation systems to optimize   recommendations for users, maximizing their engagement and satisfaction.</p> </li> <li> <p>Autonomous Vehicles: Reinforcement learning plays a crucial role in training autonomous vehicles to make   intelligent   decisions in real-world scenarios, such as lane changing, traffic signal control, and path planning.</p> </li> <li> <p>Resource Management: Reinforcement learning has been used in optimizing resource allocation and management, such   as   energy management in smart grids and traffic signal optimization.</p> </li> </ul> <p>Reinforcement learning offers an exciting and powerful approach to training intelligent agents that can adapt and learn from their environment. Embrace the challenges and rewards of reinforcement learning and unlock the potential of intelligent decision-making in your projects.</p>"},{"location":"nevarok-ml/getting-started/supported-platforms/","title":"NevarokML: Supported Platforms","text":"<p>NevarokML, the powerful machine learning plugin for Unreal Engine, supports various platforms for deploying your AI-driven projects. With NevarokML, you can create intelligent and adaptive virtual experiences that can be enjoyed by users on different devices and operating systems. Here is a list of supported platforms for NevarokML:</p> <p>Windows: NevarokML supports Windows as the primary platform for training machine learning models. You can leverage the full range of NevarokML's training capabilities on Windows machines.</p> <p>Unreal Engine Supported Platforms: Once the models are trained, they can be imported into Unreal Engine and used on any platform supported by Unreal Engine itself. For more information about supported platforms please refer to Official Unreal Engine NNE documentation</p> <p>NevarokML's compatibility with Unreal Engine's supported platforms allows you to deploy your AI-powered projects across multiple devices and operating systems, ensuring a wider reach for your applications.</p> <p>Note</p> <p>Please note that while training models is currently supported only on Windows machines, the trained models can be imported and utilized on any platform supported by Unreal Engine's Neural Network Engine (NNE). This provides flexibility in developing and deploying your machine learning projects across various platforms.</p> <p>Note</p> <p>It's important to refer to the specific documentation and guidelines provided by Unreal Engine for each platform to ensure compatibility and optimize performance when using NevarokML on different devices.</p> <p>Harness the power of NevarokML and Unreal Engine to create compelling and intelligent virtual experiences on a range of supported platforms. Expand your audience and deliver immersive AI-driven applications to users worldwide.</p>"},{"location":"nevarok-ml/getting-started/under-the-hood/","title":"NevarokML: Under the Hood","text":"<p>NevarokML is a powerful plugin designed to bring machine learning capabilities to Unreal Engine. It operates as a client-server application, where Unreal Engine serves as the server, and a Python library acts as the client. This setup enables seamless integration of machine learning functionalities into Unreal Engine projects. Here's an overview of how NevarokML works:</p> <pre><code>sequenceDiagram\n  autonumber\n  participant Unreal Engine\n  participant NevarokML Plugin\n  participant Python Library\n\n  Unreal Engine -&gt;&gt; NevarokML Plugin: Send game state and observations\n  NevarokML Plugin -&gt;&gt; Python Library: Receive game state and observations\n  Python Library -&gt;&gt; NevarokML Plugin: Perform machine learning computations\n  NevarokML Plugin -&gt;&gt; Unreal Engine: Send computed results (actions, model parameters)</code></pre>"},{"location":"nevarok-ml/getting-started/under-the-hood/#client-server-architecture","title":"Client-Server Architecture:","text":"<ul> <li>NevarokML follows a client-server architecture, with Unreal Engine as the server and the Python library as the client. This architecture facilitates the integration of machine learning capabilities into Unreal Engine projects.</li> </ul>"},{"location":"nevarok-ml/getting-started/under-the-hood/#communication-via-sockets-and-json","title":"Communication via Sockets and JSON:","text":"<ul> <li>NevarokML utilizes sockets and exchanges data in JSON format to enable communication between Unreal Engine and the Python client. This approach ensures efficient and reliable transfer of information between the server and the client.</li> </ul>"},{"location":"nevarok-ml/getting-started/under-the-hood/#training-models","title":"Training Models:","text":"<ul> <li>NevarokML supports training machine learning models using reinforcement learning algorithms. It leverages the stable-baselines3 library, a popular and reliable reinforcement learning library. With NevarokML, you can use algorithms such as Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), Deep Q-Network (DQN), Soft Actor-Critic (SAC), and Twin Delayed DDPG (TD3) to train intelligent agents.</li> </ul>"},{"location":"nevarok-ml/getting-started/under-the-hood/#integration-with-unreal-engine-neural-network-engine-nne","title":"Integration with Unreal Engine Neural Network Engine (NNE):","text":"<ul> <li>NevarokML seamlessly integrates with Unreal Engine's Neural Network Engine (NNE). It allows you to import trained models into Unreal Engine in the form of NNEModelData. These models can enhance the capabilities of Unreal Engine projects, enabling AI-driven experiences.</li> </ul> <p>Note</p> <p>It's important to note that the models are trained in the stable-baselines3 environment and are automatically converted to ONNX models upon saving. Unreal Engine supports ONNX models, ensuring compatibility and ease of use.</p>"},{"location":"nevarok-ml/getting-started/under-the-hood/#platform-compatibility","title":"Platform Compatibility:","text":"<ul> <li>While the training of models is currently supported only on Windows machines, the trained models can be imported and used on any platform supported by Unreal Engine.</li> </ul> <p>By leveraging the capabilities of NevarokML, Unreal Engine developers can harness the power of machine learning algorithms and create immersive and intelligent experiences within their projects.</p>"},{"location":"nevarok-ml/introduction/nevarokml-reinforcement-learning/","title":"How NevarokML Empowers Reinforcement Learning in Unreal Engine","text":"<p>NevarokML seamlessly integrates the power of reinforcement learning into the Unreal Engine ecosystem. By leveraging the stable-baselines3 library, NevarokML provides developers with a robust set of reinforcement learning algorithms, including PPO, A2C, DDPG, DQN, SAC, and TD3.</p> <p>With NevarokML, developers can:</p> <ul> <li> <p>Train Intelligent Agents: Create intelligent agents within Unreal Engine that can learn and adapt to dynamic   environments through reinforcement learning techniques.</p> </li> <li> <p>Optimize Decision-Making: Utilize reinforcement learning algorithms to train agents to make optimal decisions   based on   their current states and maximize cumulative rewards.</p> </li> <li> <p>Deepen Learning Capabilities: Combine reinforcement learning with deep neural networks to handle complex tasks and   achieve higher levels of performance.</p> </li> <li> <p>Import Pre-Trained Models: NevarokML supports the import of pre-trained models in the form of NNEModelData, which   seamlessly integrates with Unreal Engine's Neural Network Engine (NNE).</p> </li> </ul> <p>This plugin opens up exciting possibilities for creating adaptive and intelligent environments within Unreal Engine, pushing the boundaries of what can be achieved in the field of reinforcement learning. Unlock the full potential of your projects with NevarokML and revolutionize the way you approach machine learning in Unreal Engine.</p>"},{"location":"nevarok-ml/introduction/what-is-nevarokml/","title":"What is NevarokML?","text":"<p>NevarokML is a powerful plugin designed to bring machine learning capabilities to Unreal Engine. Built upon the stable-baselines3 library, NevarokML extends its functionality and provides developers with the tools to harness the power of reinforcement and deep reinforcement learning within their Unreal Engine projects. Whether you prefer working with C++ or Blueprints, NevarokML offers a seamless integration that allows you to leverage the potential of machine learning algorithms.</p>"},{"location":"nevarok-ml/introduction/what-is-nevarokml/#key-features","title":"Key Features","text":"<ul> <li> <p>Reinforcement Learning: NevarokML enables you to incorporate reinforcement learning techniques into your Unreal   Engine   projects. By utilizing algorithms such as Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), Deep   Deterministic Policy Gradient (DDPG), Deep Q-Network (DQN), Soft Actor-Critic (SAC), and Twin Delayed DDPG (TD3), you   can train intelligent agents to interact and learn from their environment.</p> </li> <li> <p>Stable-baselines3 Compatibility: NevarokML inherits functionality from the stable-baselines3 library, a popular   and   reliable library for reinforcement learning. By building upon stable-baselines3, NevarokML benefits from its robust   algorithms and proven techniques, ensuring stability and reliability in your machine learning workflows.</p> </li> <li> <p>Integration with Neural Network Engine (NNE): NevarokML seamlessly integrates with Unreal Engine's Neural Network   Engine (NNE). It provides support for importing models into Unreal Engine in the form of NNEModelData. This enables   you   to utilize pre-trained models and leverage their capabilities within your Unreal Engine projects, opening up a world   of   possibilities for AI-driven experiences.</p> </li> </ul>"},{"location":"nevarok-ml/introduction/what-is-nevarokml/#getting-started","title":"Getting Started","text":"<p>To get started with NevarokML, you can follow these steps:</p> <ul> <li> <p>Installation: Download and install the NevarokML plugin from the official website or the Unreal Marketplace. Follow the provided instructions to integrate the plugin into your Unreal Engine project.</p> </li> <li> <p>Documentation: Refer to the comprehensive documentation provided by NevarokML to understand the plugin's features, functionalities, and usage instructions. The documentation covers everything from setting up your environment to training and deploying machine learning agents within Unreal Engine.</p> </li> <li> <p>Examples and Tutorials: Explore the range of examples and tutorials available with NevarokML to gain hands-on experience and learn best practices. These resources provide step-by-step guidance on implementing machine learning algorithms, training agents, and integrating them into your Unreal Engine projects.</p> </li> <li> <p>Community and Support: Join the NevarokML community on Discord, where you can connect with other users, share your experiences, ask questions, and receive support. Engage in discussions, exchange ideas, and collaborate on projects to enhance your machine learning endeavors.</p> </li> </ul> <p>Start unlocking the potential of machine learning in Unreal Engine with NevarokML. Empower your projects with intelligent agents and create immersive experiences driven by AI.</p>"}]}